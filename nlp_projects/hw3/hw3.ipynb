{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim.downloader as api\n",
    "import gensim.models\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, SubsetRandomSampler, random_split\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonReviewDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        self.file_path = file_path\n",
    "        self.transform = transform\n",
    "        self._load_and_preprocess_data()\n",
    "\n",
    "    def _load_and_preprocess_data(self):\n",
    "        try:\n",
    "            amazon_data = pd.read_csv(self.file_path, sep='\\t', on_bad_lines='skip')\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found. Ensure the correct path is provided.\")\n",
    "            return\n",
    "\n",
    "        important_fields = ['star_rating', 'review_body']\n",
    "        amazon_data = amazon_data[important_fields].dropna(subset=['star_rating'])\n",
    "        amazon_data['review_body'] = amazon_data['review_body'].fillna(\"\")\n",
    "        amazon_data['Class'] = amazon_data['star_rating'].apply(lambda rating: 0 if rating in [1, 2, 3] else 1)\n",
    "\n",
    "        review_size = 50000 \n",
    "        balanced_data = pd.concat([\n",
    "            amazon_data[amazon_data['Class'] == 0].sample(n=review_size, random_state=4),\n",
    "            amazon_data[amazon_data['Class'] == 1].sample(n=review_size, random_state=4)\n",
    "        ], axis=0)\n",
    "\n",
    "        self.dataframe = balanced_data\n",
    "        self.dataframe['tokenized_review'] = balanced_data['review_body'].apply(utils.simple_preprocess)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tokenized_review = self.dataframe.iloc[index]['tokenized_review']\n",
    "        label = self.dataframe.iloc[index]['Class']\n",
    "\n",
    "        if self.transform:\n",
    "            tokenized_review = self.transform(tokenized_review)\n",
    "        return tokenized_review, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "Comparing the two approaches, it is clear that the google pretrained model does a better job of encoding semantic simlarities between words. \n",
    "<br>\n",
    "<br>\n",
    "Pretrained Model (Google):  [('queen', 0.7118193507194519)]\n",
    "<br>\n",
    "Trained Model: [('rolodex', 0.5274915099143982)]\n",
    "<br>\n",
    "\n",
    "* MAKE SURE YOU CHANGE THE FILE PATH TO WHERE DATA.TSV IS ON YOUR LOCAL MACHINE. \n",
    "* https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/buckethoop/Downloads/544HW3/hw3.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m wv \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mword2vec-google-news-300\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m res \u001b[39m=\u001b[39m wv\u001b[39m.\u001b[39mmost_similar(positive\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mwoman\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mking\u001b[39m\u001b[39m'\u001b[39m], negative \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mman\u001b[39m\u001b[39m'\u001b[39m], topn\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgoogle: \u001b[39m\u001b[39m\"\u001b[39m, res)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gensim/downloader.py:503\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    501\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, BASE_DIR)\n\u001b[1;32m    502\u001b[0m module \u001b[39m=\u001b[39m \u001b[39m__import__\u001b[39m(name)\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39;49mload_data()\n",
      "File \u001b[0;32m~/gensim-data/word2vec-google-news-300/__init__.py:8\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data\u001b[39m():\n\u001b[1;32m      7\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(base_dir, \u001b[39m'\u001b[39m\u001b[39mword2vec-google-news-300\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mword2vec-google-news-300.gz\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     model \u001b[39m=\u001b[39m KeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(path, binary\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[1;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[1;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[1;32m   1722\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gensim/models/keyedvectors.py:2065\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2062\u001b[0m kv \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(vector_size, vocab_size, dtype\u001b[39m=\u001b[39mdatatype)\n\u001b[1;32m   2064\u001b[0m \u001b[39mif\u001b[39;00m binary:\n\u001b[0;32m-> 2065\u001b[0m     _word2vec_read_binary(\n\u001b[1;32m   2066\u001b[0m         fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[1;32m   2067\u001b[0m     )\n\u001b[1;32m   2068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2069\u001b[0m     _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gensim/models/keyedvectors.py:1960\u001b[0m, in \u001b[0;36m_word2vec_read_binary\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding)\u001b[0m\n\u001b[1;32m   1958\u001b[0m new_chunk \u001b[39m=\u001b[39m fin\u001b[39m.\u001b[39mread(binary_chunk_size)\n\u001b[1;32m   1959\u001b[0m chunk \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_chunk\n\u001b[0;32m-> 1960\u001b[0m processed_words, chunk \u001b[39m=\u001b[39m _add_bytes_to_kv(\n\u001b[1;32m   1961\u001b[0m     kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\n\u001b[1;32m   1962\u001b[0m tot_processed_words \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m processed_words\n\u001b[1;32m   1963\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(new_chunk) \u001b[39m<\u001b[39m binary_chunk_size:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gensim/models/keyedvectors.py:1943\u001b[0m, in \u001b[0;36m_add_bytes_to_kv\u001b[0;34m(kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1941\u001b[0m word \u001b[39m=\u001b[39m word\u001b[39m.\u001b[39mlstrip(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1942\u001b[0m vector \u001b[39m=\u001b[39m frombuffer(chunk, offset\u001b[39m=\u001b[39mi_vector, count\u001b[39m=\u001b[39mvector_size, dtype\u001b[39m=\u001b[39mREAL)\u001b[39m.\u001b[39mastype(datatype)\n\u001b[0;32m-> 1943\u001b[0m _add_word_to_kv(kv, counts, word, vector, vocab_size)\n\u001b[1;32m   1944\u001b[0m start \u001b[39m=\u001b[39m i_vector \u001b[39m+\u001b[39m bytes_per_vector\n\u001b[1;32m   1945\u001b[0m processed_words \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gensim/models/keyedvectors.py:1911\u001b[0m, in \u001b[0;36m_add_word_to_kv\u001b[0;34m(kv, counts, word, weights, vocab_size)\u001b[0m\n\u001b[1;32m   1909\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mduplicate word \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m in word2vec file, ignoring all but first\u001b[39m\u001b[39m\"\u001b[39m, word)\n\u001b[1;32m   1910\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m-> 1911\u001b[0m word_id \u001b[39m=\u001b[39m kv\u001b[39m.\u001b[39;49madd_vector(word, weights)\n\u001b[1;32m   1913\u001b[0m \u001b[39mif\u001b[39;00m counts \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1914\u001b[0m     \u001b[39m# Most common scenario: no vocab file given. Just make up some bogus counts, in descending order.\u001b[39;00m\n\u001b[1;32m   1915\u001b[0m     \u001b[39m# TODO (someday): make this faking optional, include more realistic (Zipf-based) fake numbers.\u001b[39;00m\n\u001b[1;32m   1916\u001b[0m     word_count \u001b[39m=\u001b[39m vocab_size \u001b[39m-\u001b[39m word_id\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gensim/models/keyedvectors.py:548\u001b[0m, in \u001b[0;36mKeyedVectors.add_vector\u001b[0;34m(self, key, vector)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Add one new vector at the given key, into existing slot if available.\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \n\u001b[1;32m    529\u001b[0m \u001b[39mWarning: using this repeatedly is inefficient, requiring a full reallocation & copy,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m \n\u001b[1;32m    546\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    547\u001b[0m target_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_index\n\u001b[0;32m--> 548\u001b[0m \u001b[39mif\u001b[39;00m target_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex_to_key[target_index] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    549\u001b[0m     \u001b[39m# must append at end by expanding existing structures\u001b[39;00m\n\u001b[1;32m    550\u001b[0m     target_index \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m    551\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    552\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAdding single vectors to a KeyedVectors which grows by one each time can be costly. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    553\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider adding in batches or preallocating to the required size.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    554\u001b[0m         \u001b[39mUserWarning\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Google\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "res = wv.most_similar(positive=['woman', 'king'], negative = ['man'], topn=1)\n",
    "print(\"Pretrained Model (Google): \", res)\n",
    "\n",
    "#Me\n",
    "data_path = '/content/data.tsv'  # Replace with your actual path\n",
    "amazon_data = AmazonReviewDataset(data_path)\n",
    "\n",
    "wrd2vec_reviews = [tokens for tokens, _ in amazon_data]\n",
    "\n",
    "wrd2vec = gensim.models.Word2Vec(sentences= wrd2vec_reviews, vector_size=300, window=13, min_count=9)\n",
    "\n",
    "result_own_model = wrd2vec.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(f\"Trained Model: {result_own_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 \n",
    "\n",
    "The Word2Vec approach yields similar results than that of TFIDF. You can see the differences below:\n",
    "\n",
    "Perceptron Accuracy (Word2Vec): 79.155%\n",
    "<br>\n",
    "SVM Accuracy (Word2Vec): 80.979%\n",
    "<br>\n",
    "<br>\n",
    "TFIDF (PERC):\n",
    "Prec: 0.7817998994469583 ,Rec: 0776956130708504 , F1: 0.7793704891740175\n",
    "TFIDF \n",
    "<br>\n",
    "TFIDF (SVM):\n",
    "Prec: 0.8193973258830572 ,Rec: 0.8206255621065255 , F1: 0.8200109840730939 \n",
    "\n",
    "<br>\n",
    "*Note: I used the TFIDF values from the previous HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/buckethoop/Downloads/544HW3/hw3.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m feature_vector\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m num_features \u001b[39m=\u001b[39m \u001b[39m300\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([calculate_average_word2vec(wv, review, num_features) \u001b[39mfor\u001b[39;00m review \u001b[39min\u001b[39;00m amazon_data\u001b[39m.\u001b[39mpreprocessed_data])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m Y \u001b[39m=\u001b[39m amazon_data\u001b[39m.\u001b[39m_load_and_preprocess_data[\u001b[39m'\u001b[39m\u001b[39mClass\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues  \u001b[39m# Ensure this line matches your data structure\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Split data\u001b[39;00m\n",
      "\u001b[1;32m/Users/buckethoop/Downloads/544HW3/hw3.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m feature_vector\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m num_features \u001b[39m=\u001b[39m \u001b[39m300\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([calculate_average_word2vec(wv, review, num_features) \u001b[39mfor\u001b[39;00m review \u001b[39min\u001b[39;00m amazon_data\u001b[39m.\u001b[39mpreprocessed_data])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m Y \u001b[39m=\u001b[39m amazon_data\u001b[39m.\u001b[39m_load_and_preprocess_data[\u001b[39m'\u001b[39m\u001b[39mClass\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues  \u001b[39m# Ensure this line matches your data structure\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Split data\u001b[39;00m\n",
      "\u001b[1;32m/Users/buckethoop/Downloads/544HW3/hw3.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m feature_vector \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((num_features,), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m n_words \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m index2word_set \u001b[39m=\u001b[39m \u001b[39mset\u001b[39;49m(word2vec_model\u001b[39m.\u001b[39;49mindex_to_key)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m text:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/buckethoop/Downloads/544HW3/hw3.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m index2word_set:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def avg_word2vec(review, w2v, num_features=300):\n",
    "    feature_vec = np.zeros((num_features,), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in review:\n",
    "        if word in w2v:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, w2v[word])\n",
    "    if n_words:\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "\n",
    "data_features = np.array([avg_word2vec(review, wv) for review, _ in amazon_data])\n",
    "labels = np.array([label for _, label in amazon_data])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_features, labels, test_size=0.2, random_state=30)\n",
    "\n",
    "perc = Perceptron(max_iter=5000)\n",
    "perc.fit(X_train, y_train)\n",
    "perc_pred = perc.predict(X_test)\n",
    "\n",
    "svm = LinearSVC(max_iter=1000)\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "\n",
    "\n",
    "print(f\"Perceptron Accuracy (Word2Vec):, {100 * accuracy_score(y_test, perc_pred)}%\")\n",
    "print(f\"SVM Accuracy (Word2Vec):, {100 * accuracy_score(y_test, svm_pred)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a\n",
    "\n",
    "My accuracy for the section was: 82.72%\n",
    "<br>\n",
    "* https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = [avg_word2vec(review, wv) for review, _ in amazon_data]\n",
    "labels = [label for _, label in amazon_data]\n",
    "\n",
    "features_tensor = torch.tensor(data_features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 50)\n",
    "        self.fc2 = nn.Linear(50, 5)\n",
    "        self.fc3 = nn.Linear(5, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b\n",
    "\n",
    "The accuracy value for this section was: 72.43%\n",
    "\n",
    "<br>\n",
    "Comparing the accuracies from 4a and 4b you can see that the concatenation approach doesn't perform as well as the average Word2Vec approach.\n",
    "I think that this is because when we only consider the first 10 Word2Vec vectors we run the risk of losing information. With the average vectors approach we get a more holistic look at that information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_vector(review, w2v_model, max_len=10):\n",
    "    vectors = []\n",
    "    for word in review:\n",
    "        if word in w2v_model.wv:\n",
    "            vectors.append(w2v_model.wv[word])\n",
    "    \n",
    "    if len(vectors) < max_len:\n",
    "        vectors.extend([np.zeros(w2v_model.vector_size) for _ in range(max_len - len(vectors))])\n",
    "    else:\n",
    "        vectors = vectors[:max_len]\n",
    "        \n",
    "    return np.concatenate(vectors, axis=0)\n",
    "\n",
    "X = [review_to_vector(review, wrd2vec) for review, _ in amazon_data]  # Note: Using wrd2vec model\n",
    "y = [label for _, label in amazon_data]\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1=50, hidden_size2=5, output_size=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "input_size = wrd2vec.vector_size * 10  # 10 concatenated Word2Vec vectors\n",
    "mlp_model = MLP(input_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = mlp_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5a\n",
    "\n",
    "My accuracy in this section is: 81.8%\n",
    "<br>\n",
    "* https://pytorch.org/tutorials/intermediate/char_rnn_classification_\n",
    "tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_vector(review, w2v_model, max_len=10):\n",
    "    vectors = [w2v_model.wv[word] if word in w2v_model.wv else np.zeros(w2v_model.vector_size) for word in review[:max_len]]\n",
    "    while len(vectors) < max_len:\n",
    "        vectors.append(np.zeros(w2v_model.vector_size))\n",
    "    return np.array(vectors)\n",
    "\n",
    "X = [review_to_vector(review, wrd2vec) for review, _ in amazon_data]\n",
    "y = [label for _, label in amazon_data]\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)  # Initial hidden state\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # Use last sequence output as input to FC layer\n",
    "        return out\n",
    "\n",
    "input_size = wrd2vec.vector_size\n",
    "hidden_size = 10\n",
    "output_size = 2\n",
    "\n",
    "model = RNNModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5b\n",
    "\n",
    "My accuracy in this section is: 83.24%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_vector(review, w2v_model, max_len=10):\n",
    "    vectors = [w2v_model.wv[word] if word in w2v_model.wv else np.zeros(w2v_model.vector_size) for word in review[:max_len]]\n",
    "    while len(vectors) < max_len:\n",
    "        vectors.append(np.zeros(w2v_model.vector_size))\n",
    "    return np.array(vectors)\n",
    "\n",
    "X = [review_to_vector(review, wrd2vec) for review, _ in amazon_data]\n",
    "y = [label for _, label in amazon_data]\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)  # Initial hidden state\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # Use last sequence output as input to FC layer\n",
    "        return out\n",
    "\n",
    "input_size = wrd2vec.vector_size\n",
    "hidden_size = 10\n",
    "output_size = 2\n",
    "\n",
    "model = GRUModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy using GRU: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5c \n",
    "\n",
    "My accuracy in this section is: 82.725%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_vector(review, w2v_model, max_len=10):\n",
    "    vectors = [w2v_model.wv[word] if word in w2v_model.wv else np.zeros(w2v_model.vector_size) for word in review[:max_len]]\n",
    "    while len(vectors) < max_len:\n",
    "        vectors.append(np.zeros(w2v_model.vector_size))\n",
    "    return np.array(vectors)\n",
    "\n",
    "X = [review_to_vector(review, wrd2vec) for review, _ in amazon_data]\n",
    "y = [label for _, label in amazon_data]\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)  # Initial hidden state\n",
    "        c0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)  # Initial cell state\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Use last sequence output as input to FC layer\n",
    "        return out\n",
    "\n",
    "input_size = wrd2vec.vector_size\n",
    "hidden_size = 10\n",
    "output_size = 2\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy using LSTM: {100 * correct / total}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
